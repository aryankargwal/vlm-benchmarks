{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "import base64\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMAGE_DIR = 'data/images'\n",
    "JSON_FILE = 'data/arxivqa.jsonl'\n",
    "\n",
    "# Load the dataset from JSON Lines file\n",
    "with open(JSON_FILE, 'r') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert dataset to DataFrame for easier handling\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Load a pre-trained model for sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Randomly select 1000 samples\n",
    "random_samples = random.sample(range(len(df)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode image data to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image = Image.open(f)\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")  # Save the image to a buffer\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "def query_model(instruction, base64_image):\n",
    "    system_prompt = (\n",
    "        \"Answer Query Based on Image\"\n",
    "    )\n",
    "    \n",
    "    full_instruction = f\"{system_prompt}\\n\\n{instruction}\"\n",
    "    \n",
    "    url = \"https://proxy.tune.app/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"YOUR_TUNE_API_KEY\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"temperature\": 0.4,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": full_instruction},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"mistral/pixtral-12B-2409\",\n",
    "        \"stream\": False,\n",
    "        \"frequency_penalty\": 0.2,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "valid_samples = []\n",
    "similarity_scores = []\n",
    "rationale_scores = []\n",
    "\n",
    "# Process dataset and accumulate results\n",
    "for idx in random_samples:  # Use idx to get each sample\n",
    "    sample = df.iloc[idx]  # Access the sample using the index\n",
    "    filename = sample.get('image', 'No filename available')\n",
    "    image_path = os.path.join(IMAGE_DIR, filename)\n",
    "    \n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Image file not found: {image_path}, skipping...\")\n",
    "        continue  # Skip if image file is not found\n",
    "    \n",
    "    # Encode image to base64\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    # Prepare instruction\n",
    "    instruction = f\"Question: {sample.get('question', '')}\\nOptions: {', '.join(sample.get('options', []))}\"\n",
    "    \n",
    "    # Query the model\n",
    "    response = query_model(instruction, base64_image)\n",
    "    \n",
    "    # Extract model outputs\n",
    "    choices = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip().split(\"\\n\")\n",
    "    model_label = choices[0] if len(choices) > 0 else \"\"\n",
    "    model_rationale = \"\\n\".join(choices[1:]) if len(choices) > 1 else \"\"\n",
    "    \n",
    "    # Extract reference outputs\n",
    "    reference_label = sample.get('label', 'No label available')\n",
    "    reference_rationale = sample.get('rationale', 'No rationale available')\n",
    "    \n",
    "    # Compute embeddings for reference outputs and model outputs\n",
    "    label_embedding = model.encode(reference_label, convert_to_tensor=True)\n",
    "    model_label_embedding = model.encode(model_label, convert_to_tensor=True)\n",
    "    rationale_embedding = model.encode(reference_rationale, convert_to_tensor=True)\n",
    "    model_rationale_embedding = model.encode(model_rationale, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    label_similarity = util.pytorch_cos_sim(model_label_embedding, label_embedding).item()\n",
    "    rationale_similarity = util.pytorch_cos_sim(model_rationale_embedding, rationale_embedding).item()\n",
    "    \n",
    "    # Collect the results\n",
    "    valid_samples.append({\n",
    "        \"filename\": filename,\n",
    "        \"instruction\": instruction,\n",
    "        \"model_label\": model_label,\n",
    "        \"model_rationale\": model_rationale,\n",
    "        \"reference_label\": reference_label,\n",
    "        \"reference_rationale\": reference_rationale,\n",
    "        \"label_similarity\": label_similarity,\n",
    "        \"rationale_similarity\": rationale_similarity\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results and metrics saved.\n"
     ]
    }
   ],
   "source": [
    "# Calculate win rate based on similarity thresholds (e.g., 0.8)\n",
    "label_win_rate = sum(score > 0.8 for score in [sample['label_similarity'] for sample in valid_samples]) / len(valid_samples) if len(valid_samples) > 0 else 0\n",
    "rationale_win_rate = sum(score > 0.8 for score in [sample['rationale_similarity'] for sample in valid_samples]) / len(valid_samples) if len(valid_samples) > 0 else 0\n",
    "\n",
    "# Calculate top-1, top-5, and top-10 average scores\n",
    "sorted_label_scores = sorted([sample['label_similarity'] for sample in valid_samples], reverse=True)\n",
    "top_1_label_score = sorted_label_scores[0] if len(sorted_label_scores) >= 1 else 0\n",
    "top_5_label_average = sum(sorted_label_scores[:5]) / 5 if len(sorted_label_scores) >= 5 else 0\n",
    "top_10_label_average = sum(sorted_label_scores[:10]) / 10 if len(sorted_label_scores) >= 10 else 0\n",
    "\n",
    "sorted_rationale_scores = sorted([sample['rationale_similarity'] for sample in valid_samples], reverse=True)\n",
    "top_1_rationale_score = sorted_rationale_scores[0] if len(sorted_rationale_scores) >= 1 else 0\n",
    "top_5_rationale_average = sum(sorted_rationale_scores[:5]) / 5 if len(sorted_rationale_scores) >= 5 else 0\n",
    "top_10_rationale_average = sum(sorted_rationale_scores[:10]) / 10 if len(sorted_rationale_scores) >= 10 else 0\n",
    "\n",
    "# Save inference results to a file\n",
    "results_df = pd.DataFrame(valid_samples)\n",
    "results_df.to_csv('arxivqa_model_results.csv', index=False)\n",
    "\n",
    "# Save metrics to a file\n",
    "metrics_df = pd.DataFrame([{\n",
    "    \"label_win_rate\": label_win_rate,\n",
    "    \"rationale_win_rate\": rationale_win_rate,\n",
    "    \"top_1_label_score\": top_1_label_score,\n",
    "    \"top_5_label_average_score\": top_5_label_average,\n",
    "    \"top_10_label_average_score\": top_10_label_average,\n",
    "    \"top_1_rationale_score\": top_1_rationale_score,\n",
    "    \"top_5_rationale_average_score\": top_5_rationale_average,\n",
    "    \"top_10_rationale_average_score\": top_10_rationale_average\n",
    "}])\n",
    "metrics_df.to_csv('arxivqa_dataset_scores.csv', index=False)\n",
    "\n",
    "print(\"Evaluation complete. Results and metrics saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
