{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util  # For cosine similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105574339eb54dcd9b8c4a66e3b94c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10decf6b3f314edab3301cf723d3fe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryankargwal/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1614: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"mlfoundations/VisIT-Bench\", split='test')\n",
    "\n",
    "# Load a pre-trained model for sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(instruction, image_url):\n",
    "    # Define a system prompt that sets the limit for the description\n",
    "    system_prompt = \"Describe this image\"\n",
    "    \n",
    "    # Combine the system prompt with the user's instruction\n",
    "    full_instruction = f\"{system_prompt}\"\n",
    "    \n",
    "    url = \"https://proxy.tune.app/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"YOUR_TUNE_API_KEY\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"temperature\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": full_instruction},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"mistral/pixtral-12B-2409\",\n",
    "        \"stream\": False,\n",
    "        \"frequency_penalty\": 0.2,\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        #print(f\"Error querying model: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "valid_samples = []\n",
    "win_count = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Process dataset and accumulate results\n",
    "for sample in dataset:\n",
    "    # Extract the instruction and image URL\n",
    "    instruction = sample.get('instruction', 'No instruction available')\n",
    "    public_images_metadata = sample.get('public_images_metadata', '{}')\n",
    "    if isinstance(public_images_metadata, str):\n",
    "        try:\n",
    "            public_images_metadata = json.loads(public_images_metadata)\n",
    "        except json.JSONDecodeError:\n",
    "            #print(\"Error decoding JSON from 'public_images_metadata', skipping...\")\n",
    "            continue  # Skip the sample if metadata is invalid\n",
    "    \n",
    "    image_url = public_images_metadata.get('OriginalURL', None)\n",
    "    if not image_url:\n",
    "        #print(\"No valid image URL found, skipping...\")\n",
    "        continue  # Skip if no image URL is found\n",
    "\n",
    "    # Query the model\n",
    "    response = query_model(instruction, image_url)\n",
    "    if not response or not response.get(\"choices\"):\n",
    "        #print(f\"Invalid model response for instruction: {instruction}, skipping...\")\n",
    "        continue  # Skip if model response is invalid\n",
    "\n",
    "    model_output = response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    if not model_output:\n",
    "        #print(\"No model output, skipping...\")\n",
    "        continue  # Skip if no model output\n",
    "\n",
    "    # Extract reference output\n",
    "    reference_output = sample.get('instruction_conditioned_caption', 'No reference output available')\n",
    "    if not reference_output:\n",
    "        #print(\"No reference output, skipping...\")\n",
    "        continue  # Skip if no reference output\n",
    "\n",
    "    # Encode the texts using a sentence embedding model\n",
    "    reference_embedding = model.encode(reference_output, convert_to_tensor=True)\n",
    "    model_embedding = model.encode(model_output, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_score = util.pytorch_cos_sim(model_embedding, reference_embedding).item()\n",
    "\n",
    "    # Increment win count based on a similarity threshold (e.g., if similarity > 0.8)\n",
    "    if similarity_score > 0.8:\n",
    "        win_count += 1\n",
    "    \n",
    "    total_samples += 1\n",
    "\n",
    "    # Append valid data to the list\n",
    "    valid_samples.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"reference_output\": reference_output,\n",
    "        \"model_output\": model_output,\n",
    "        \"similarity_score\": similarity_score\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate, average similarity, and top-k scores saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate win rate\n",
    "win_rate = win_count / total_samples if total_samples > 0 else 0\n",
    "\n",
    "# Calculate additional metrics\n",
    "similarity_scores = [sample['similarity_score'] for sample in valid_samples]\n",
    "\n",
    "# Average similarity score\n",
    "average_similarity = sum(similarity_scores) / total_samples\n",
    "\n",
    "# Sort scores to find top-k scores\n",
    "sorted_scores = sorted(similarity_scores, reverse=True)\n",
    "\n",
    "# Top 1, Top 5, and Top 10 average scores\n",
    "top_1_score = sorted_scores[0] if total_samples >= 1 else 0\n",
    "top_5_average = sum(sorted_scores[:5]) / 5 if total_samples >= 5 else 0\n",
    "top_10_average = sum(sorted_scores[:10]) / 10 if total_samples >= 10 else 0\n",
    "\n",
    "# Save results only for valid samples\n",
    "results_df = pd.DataFrame(valid_samples)\n",
    "results_df.to_csv('visit_caption.csv', index=False)\n",
    "\n",
    "# Save metrics to a file\n",
    "metrics_df = pd.DataFrame([{\n",
    "    \"win_rate\": win_rate,\n",
    "    \"average_similarity\": average_similarity,\n",
    "    \"top_1_score\": top_1_score,\n",
    "    \"top_5_average_score\": top_5_average,\n",
    "    \"top_10_average_score\": top_10_average\n",
    "}])\n",
    "\n",
    "metrics_df.to_csv('visit_caption_score.csv', index=False)\n",
    "\n",
    "print(\"Win rate, average similarity, and top-k scores saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
